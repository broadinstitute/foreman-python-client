# coding: utf-8

"""
    Foreman (params in:formData)

     <p>Foreman API v2 is currently the default API version.</p> 

    The version of the OpenAPI document: v2
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import io
import warnings

from pydantic import validate_call, Field, StrictFloat, StrictStr, StrictInt
from typing import Dict, List, Optional, Tuple, Union, Any

try:
    from typing import Annotated
except ImportError:
    from typing_extensions import Annotated

from pydantic import Field
from typing_extensions import Annotated
from pydantic import StrictBool, StrictFloat, StrictInt, StrictStr, field_validator

from typing import Optional, Union


from foreman.api_client import ApiClient
from foreman.api_response import ApiResponse
from foreman.rest import RESTResponseType


class ContentExportIncrementalsApi:
    """NOTE: This class is auto generated by OpenAPI Generator
    Ref: https://openapi-generator.tech

    Do not edit the class manually.
    """

    def __init__(self, api_client=None) -> None:
        if api_client is None:
            api_client = ApiClient.get_default()
        self.api_client = api_client

    @validate_call
    def post_content_export_incrementals_library(
        self,
        organization_id: Annotated[
            Union[StrictFloat, StrictInt], Field(description="Organization identifier")
        ],
        fail_on_missing_content: Annotated[
            Optional[StrictBool],
            Field(
                description="Fails if any of the repositories belonging to this organization are unexportable. False by default."
            ),
        ] = None,
        destination_server: Annotated[
            Optional[StrictStr], Field(description="Destination Server name")
        ] = None,
        chunk_size_gb: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Split the exported content into archives no greater than the specified size in gigabytes."
            ),
        ] = None,
        format: Annotated[
            Optional[StrictStr],
            Field(
                description="Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories."
            ),
        ] = None,
        from_history_id: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Export history identifier used for incremental export. If not provided the most recent export history will be used."
            ),
        ] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)], Annotated[StrictFloat, Field(gt=0)]
            ],
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> None:
        """Performs an incremental-export of the repositories in library.


        :param organization_id: Organization identifier (required)
        :type organization_id: float
        :param fail_on_missing_content: Fails if any of the repositories belonging to this organization are unexportable. False by default.
        :type fail_on_missing_content: bool
        :param destination_server: Destination Server name
        :type destination_server: str
        :param chunk_size_gb: Split the exported content into archives no greater than the specified size in gigabytes.
        :type chunk_size_gb: float
        :param format: Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories.
        :type format: str
        :param from_history_id: Export history identifier used for incremental export. If not provided the most recent export history will be used.
        :type from_history_id: float
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """  # noqa: E501

        _param = self._post_content_export_incrementals_library_serialize(
            organization_id=organization_id,
            fail_on_missing_content=fail_on_missing_content,
            destination_server=destination_server,
            chunk_size_gb=chunk_size_gb,
            format=format,
            from_history_id=from_history_id,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index,
        )

        _response_types_map: Dict[str, Optional[str]] = {
            "200": None,
        }
        response_data = self.api_client.call_api(
            *_param, _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        ).data

    @validate_call
    def post_content_export_incrementals_library_with_http_info(
        self,
        organization_id: Annotated[
            Union[StrictFloat, StrictInt], Field(description="Organization identifier")
        ],
        fail_on_missing_content: Annotated[
            Optional[StrictBool],
            Field(
                description="Fails if any of the repositories belonging to this organization are unexportable. False by default."
            ),
        ] = None,
        destination_server: Annotated[
            Optional[StrictStr], Field(description="Destination Server name")
        ] = None,
        chunk_size_gb: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Split the exported content into archives no greater than the specified size in gigabytes."
            ),
        ] = None,
        format: Annotated[
            Optional[StrictStr],
            Field(
                description="Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories."
            ),
        ] = None,
        from_history_id: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Export history identifier used for incremental export. If not provided the most recent export history will be used."
            ),
        ] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)], Annotated[StrictFloat, Field(gt=0)]
            ],
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> ApiResponse[None]:
        """Performs an incremental-export of the repositories in library.


        :param organization_id: Organization identifier (required)
        :type organization_id: float
        :param fail_on_missing_content: Fails if any of the repositories belonging to this organization are unexportable. False by default.
        :type fail_on_missing_content: bool
        :param destination_server: Destination Server name
        :type destination_server: str
        :param chunk_size_gb: Split the exported content into archives no greater than the specified size in gigabytes.
        :type chunk_size_gb: float
        :param format: Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories.
        :type format: str
        :param from_history_id: Export history identifier used for incremental export. If not provided the most recent export history will be used.
        :type from_history_id: float
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """  # noqa: E501

        _param = self._post_content_export_incrementals_library_serialize(
            organization_id=organization_id,
            fail_on_missing_content=fail_on_missing_content,
            destination_server=destination_server,
            chunk_size_gb=chunk_size_gb,
            format=format,
            from_history_id=from_history_id,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index,
        )

        _response_types_map: Dict[str, Optional[str]] = {
            "200": None,
        }
        response_data = self.api_client.call_api(
            *_param, _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        )

    @validate_call
    def post_content_export_incrementals_library_without_preload_content(
        self,
        organization_id: Annotated[
            Union[StrictFloat, StrictInt], Field(description="Organization identifier")
        ],
        fail_on_missing_content: Annotated[
            Optional[StrictBool],
            Field(
                description="Fails if any of the repositories belonging to this organization are unexportable. False by default."
            ),
        ] = None,
        destination_server: Annotated[
            Optional[StrictStr], Field(description="Destination Server name")
        ] = None,
        chunk_size_gb: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Split the exported content into archives no greater than the specified size in gigabytes."
            ),
        ] = None,
        format: Annotated[
            Optional[StrictStr],
            Field(
                description="Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories."
            ),
        ] = None,
        from_history_id: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Export history identifier used for incremental export. If not provided the most recent export history will be used."
            ),
        ] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)], Annotated[StrictFloat, Field(gt=0)]
            ],
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> RESTResponseType:
        """Performs an incremental-export of the repositories in library.


        :param organization_id: Organization identifier (required)
        :type organization_id: float
        :param fail_on_missing_content: Fails if any of the repositories belonging to this organization are unexportable. False by default.
        :type fail_on_missing_content: bool
        :param destination_server: Destination Server name
        :type destination_server: str
        :param chunk_size_gb: Split the exported content into archives no greater than the specified size in gigabytes.
        :type chunk_size_gb: float
        :param format: Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories.
        :type format: str
        :param from_history_id: Export history identifier used for incremental export. If not provided the most recent export history will be used.
        :type from_history_id: float
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """  # noqa: E501

        _param = self._post_content_export_incrementals_library_serialize(
            organization_id=organization_id,
            fail_on_missing_content=fail_on_missing_content,
            destination_server=destination_server,
            chunk_size_gb=chunk_size_gb,
            format=format,
            from_history_id=from_history_id,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index,
        )

        _response_types_map: Dict[str, Optional[str]] = {
            "200": None,
        }
        response_data = self.api_client.call_api(
            *_param, _request_timeout=_request_timeout
        )
        return response_data.response

    def _post_content_export_incrementals_library_serialize(
        self,
        organization_id,
        fail_on_missing_content,
        destination_server,
        chunk_size_gb,
        format,
        from_history_id,
        _request_auth,
        _content_type,
        _headers,
        _host_index,
    ) -> Tuple:
        _host = None

        _collection_formats: Dict[str, str] = {}

        _path_params: Dict[str, str] = {}
        _query_params: List[Tuple[str, str]] = []
        _header_params: Dict[str, Optional[str]] = _headers or {}
        _form_params: List[Tuple[str, str]] = []
        _files: Dict[str, str] = {}
        _body_params: Optional[bytes] = None

        # process the path parameters
        # process the query parameters
        # process the header parameters
        # process the form parameters
        if organization_id is not None:
            _form_params.append(("organization_id", organization_id))
        if fail_on_missing_content is not None:
            _form_params.append(("fail_on_missing_content", fail_on_missing_content))
        if destination_server is not None:
            _form_params.append(("destination_server", destination_server))
        if chunk_size_gb is not None:
            _form_params.append(("chunk_size_gb", chunk_size_gb))
        if format is not None:
            _form_params.append(("format", format))
        if from_history_id is not None:
            _form_params.append(("from_history_id", from_history_id))
        # process the body parameter

        # set the HTTP header `Content-Type`
        if _content_type:
            _header_params["Content-Type"] = _content_type
        else:
            _default_content_type = self.api_client.select_header_content_type(
                ["application/x-www-form-urlencoded", "multipart/form-data"]
            )
            if _default_content_type is not None:
                _header_params["Content-Type"] = _default_content_type

        # authentication setting
        _auth_settings: List[str] = []

        return self.api_client.param_serialize(
            method="POST",
            resource_path="/content_export_incrementals/library",
            path_params=_path_params,
            query_params=_query_params,
            header_params=_header_params,
            body=_body_params,
            post_params=_form_params,
            files=_files,
            auth_settings=_auth_settings,
            collection_formats=_collection_formats,
            _host=_host,
            _request_auth=_request_auth,
        )

    @validate_call
    def post_content_export_incrementals_repository(
        self,
        id: Annotated[
            Union[StrictFloat, StrictInt], Field(description="Repository identifier")
        ],
        chunk_size_gb: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Split the exported content into archives no greater than the specified size in gigabytes."
            ),
        ] = None,
        format: Annotated[
            Optional[StrictStr],
            Field(
                description="Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories."
            ),
        ] = None,
        from_history_id: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Export history identifier used for incremental export. If not provided the most recent export history will be used."
            ),
        ] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)], Annotated[StrictFloat, Field(gt=0)]
            ],
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> None:
        """Performs a incremental-export of the repository in library.


        :param id: Repository identifier (required)
        :type id: float
        :param chunk_size_gb: Split the exported content into archives no greater than the specified size in gigabytes.
        :type chunk_size_gb: float
        :param format: Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories.
        :type format: str
        :param from_history_id: Export history identifier used for incremental export. If not provided the most recent export history will be used.
        :type from_history_id: float
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """  # noqa: E501

        _param = self._post_content_export_incrementals_repository_serialize(
            id=id,
            chunk_size_gb=chunk_size_gb,
            format=format,
            from_history_id=from_history_id,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index,
        )

        _response_types_map: Dict[str, Optional[str]] = {
            "200": None,
        }
        response_data = self.api_client.call_api(
            *_param, _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        ).data

    @validate_call
    def post_content_export_incrementals_repository_with_http_info(
        self,
        id: Annotated[
            Union[StrictFloat, StrictInt], Field(description="Repository identifier")
        ],
        chunk_size_gb: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Split the exported content into archives no greater than the specified size in gigabytes."
            ),
        ] = None,
        format: Annotated[
            Optional[StrictStr],
            Field(
                description="Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories."
            ),
        ] = None,
        from_history_id: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Export history identifier used for incremental export. If not provided the most recent export history will be used."
            ),
        ] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)], Annotated[StrictFloat, Field(gt=0)]
            ],
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> ApiResponse[None]:
        """Performs a incremental-export of the repository in library.


        :param id: Repository identifier (required)
        :type id: float
        :param chunk_size_gb: Split the exported content into archives no greater than the specified size in gigabytes.
        :type chunk_size_gb: float
        :param format: Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories.
        :type format: str
        :param from_history_id: Export history identifier used for incremental export. If not provided the most recent export history will be used.
        :type from_history_id: float
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """  # noqa: E501

        _param = self._post_content_export_incrementals_repository_serialize(
            id=id,
            chunk_size_gb=chunk_size_gb,
            format=format,
            from_history_id=from_history_id,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index,
        )

        _response_types_map: Dict[str, Optional[str]] = {
            "200": None,
        }
        response_data = self.api_client.call_api(
            *_param, _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        )

    @validate_call
    def post_content_export_incrementals_repository_without_preload_content(
        self,
        id: Annotated[
            Union[StrictFloat, StrictInt], Field(description="Repository identifier")
        ],
        chunk_size_gb: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Split the exported content into archives no greater than the specified size in gigabytes."
            ),
        ] = None,
        format: Annotated[
            Optional[StrictStr],
            Field(
                description="Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories."
            ),
        ] = None,
        from_history_id: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Export history identifier used for incremental export. If not provided the most recent export history will be used."
            ),
        ] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)], Annotated[StrictFloat, Field(gt=0)]
            ],
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> RESTResponseType:
        """Performs a incremental-export of the repository in library.


        :param id: Repository identifier (required)
        :type id: float
        :param chunk_size_gb: Split the exported content into archives no greater than the specified size in gigabytes.
        :type chunk_size_gb: float
        :param format: Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories.
        :type format: str
        :param from_history_id: Export history identifier used for incremental export. If not provided the most recent export history will be used.
        :type from_history_id: float
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """  # noqa: E501

        _param = self._post_content_export_incrementals_repository_serialize(
            id=id,
            chunk_size_gb=chunk_size_gb,
            format=format,
            from_history_id=from_history_id,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index,
        )

        _response_types_map: Dict[str, Optional[str]] = {
            "200": None,
        }
        response_data = self.api_client.call_api(
            *_param, _request_timeout=_request_timeout
        )
        return response_data.response

    def _post_content_export_incrementals_repository_serialize(
        self,
        id,
        chunk_size_gb,
        format,
        from_history_id,
        _request_auth,
        _content_type,
        _headers,
        _host_index,
    ) -> Tuple:
        _host = None

        _collection_formats: Dict[str, str] = {}

        _path_params: Dict[str, str] = {}
        _query_params: List[Tuple[str, str]] = []
        _header_params: Dict[str, Optional[str]] = _headers or {}
        _form_params: List[Tuple[str, str]] = []
        _files: Dict[str, str] = {}
        _body_params: Optional[bytes] = None

        # process the path parameters
        # process the query parameters
        # process the header parameters
        # process the form parameters
        if id is not None:
            _form_params.append(("id", id))
        if chunk_size_gb is not None:
            _form_params.append(("chunk_size_gb", chunk_size_gb))
        if format is not None:
            _form_params.append(("format", format))
        if from_history_id is not None:
            _form_params.append(("from_history_id", from_history_id))
        # process the body parameter

        # set the HTTP header `Content-Type`
        if _content_type:
            _header_params["Content-Type"] = _content_type
        else:
            _default_content_type = self.api_client.select_header_content_type(
                ["application/x-www-form-urlencoded", "multipart/form-data"]
            )
            if _default_content_type is not None:
                _header_params["Content-Type"] = _default_content_type

        # authentication setting
        _auth_settings: List[str] = []

        return self.api_client.param_serialize(
            method="POST",
            resource_path="/content_export_incrementals/repository",
            path_params=_path_params,
            query_params=_query_params,
            header_params=_header_params,
            body=_body_params,
            post_params=_form_params,
            files=_files,
            auth_settings=_auth_settings,
            collection_formats=_collection_formats,
            _host=_host,
            _request_auth=_request_auth,
        )

    @validate_call
    def post_content_export_incrementals_version(
        self,
        id: Annotated[
            Union[StrictFloat, StrictInt],
            Field(description="Content view version identifier"),
        ],
        fail_on_missing_content: Annotated[
            Optional[StrictBool],
            Field(
                description="Fails if any of the repositories belonging to this version are unexportable. False by default."
            ),
        ] = None,
        destination_server: Annotated[
            Optional[StrictStr], Field(description="Destination Server name")
        ] = None,
        chunk_size_gb: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Split the exported content into archives no greater than the specified size in gigabytes."
            ),
        ] = None,
        format: Annotated[
            Optional[StrictStr],
            Field(
                description="Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories."
            ),
        ] = None,
        from_history_id: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Export history identifier used for incremental export. If not provided the most recent export history will be used."
            ),
        ] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)], Annotated[StrictFloat, Field(gt=0)]
            ],
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> None:
        """Performs an incremental-export of a content view version.


        :param id: Content view version identifier (required)
        :type id: float
        :param fail_on_missing_content: Fails if any of the repositories belonging to this version are unexportable. False by default.
        :type fail_on_missing_content: bool
        :param destination_server: Destination Server name
        :type destination_server: str
        :param chunk_size_gb: Split the exported content into archives no greater than the specified size in gigabytes.
        :type chunk_size_gb: float
        :param format: Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories.
        :type format: str
        :param from_history_id: Export history identifier used for incremental export. If not provided the most recent export history will be used.
        :type from_history_id: float
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """  # noqa: E501

        _param = self._post_content_export_incrementals_version_serialize(
            id=id,
            fail_on_missing_content=fail_on_missing_content,
            destination_server=destination_server,
            chunk_size_gb=chunk_size_gb,
            format=format,
            from_history_id=from_history_id,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index,
        )

        _response_types_map: Dict[str, Optional[str]] = {
            "200": None,
        }
        response_data = self.api_client.call_api(
            *_param, _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        ).data

    @validate_call
    def post_content_export_incrementals_version_with_http_info(
        self,
        id: Annotated[
            Union[StrictFloat, StrictInt],
            Field(description="Content view version identifier"),
        ],
        fail_on_missing_content: Annotated[
            Optional[StrictBool],
            Field(
                description="Fails if any of the repositories belonging to this version are unexportable. False by default."
            ),
        ] = None,
        destination_server: Annotated[
            Optional[StrictStr], Field(description="Destination Server name")
        ] = None,
        chunk_size_gb: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Split the exported content into archives no greater than the specified size in gigabytes."
            ),
        ] = None,
        format: Annotated[
            Optional[StrictStr],
            Field(
                description="Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories."
            ),
        ] = None,
        from_history_id: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Export history identifier used for incremental export. If not provided the most recent export history will be used."
            ),
        ] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)], Annotated[StrictFloat, Field(gt=0)]
            ],
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> ApiResponse[None]:
        """Performs an incremental-export of a content view version.


        :param id: Content view version identifier (required)
        :type id: float
        :param fail_on_missing_content: Fails if any of the repositories belonging to this version are unexportable. False by default.
        :type fail_on_missing_content: bool
        :param destination_server: Destination Server name
        :type destination_server: str
        :param chunk_size_gb: Split the exported content into archives no greater than the specified size in gigabytes.
        :type chunk_size_gb: float
        :param format: Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories.
        :type format: str
        :param from_history_id: Export history identifier used for incremental export. If not provided the most recent export history will be used.
        :type from_history_id: float
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """  # noqa: E501

        _param = self._post_content_export_incrementals_version_serialize(
            id=id,
            fail_on_missing_content=fail_on_missing_content,
            destination_server=destination_server,
            chunk_size_gb=chunk_size_gb,
            format=format,
            from_history_id=from_history_id,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index,
        )

        _response_types_map: Dict[str, Optional[str]] = {
            "200": None,
        }
        response_data = self.api_client.call_api(
            *_param, _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        )

    @validate_call
    def post_content_export_incrementals_version_without_preload_content(
        self,
        id: Annotated[
            Union[StrictFloat, StrictInt],
            Field(description="Content view version identifier"),
        ],
        fail_on_missing_content: Annotated[
            Optional[StrictBool],
            Field(
                description="Fails if any of the repositories belonging to this version are unexportable. False by default."
            ),
        ] = None,
        destination_server: Annotated[
            Optional[StrictStr], Field(description="Destination Server name")
        ] = None,
        chunk_size_gb: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Split the exported content into archives no greater than the specified size in gigabytes."
            ),
        ] = None,
        format: Annotated[
            Optional[StrictStr],
            Field(
                description="Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories."
            ),
        ] = None,
        from_history_id: Annotated[
            Optional[Union[StrictFloat, StrictInt]],
            Field(
                description="Export history identifier used for incremental export. If not provided the most recent export history will be used."
            ),
        ] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)], Annotated[StrictFloat, Field(gt=0)]
            ],
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> RESTResponseType:
        """Performs an incremental-export of a content view version.


        :param id: Content view version identifier (required)
        :type id: float
        :param fail_on_missing_content: Fails if any of the repositories belonging to this version are unexportable. False by default.
        :type fail_on_missing_content: bool
        :param destination_server: Destination Server name
        :type destination_server: str
        :param chunk_size_gb: Split the exported content into archives no greater than the specified size in gigabytes.
        :type chunk_size_gb: float
        :param format: Export formats.Choose syncable if the exported content needs to be in a yum format. This option is only available for yum, file repositories. Choose importable if the importing server uses the same version  and exported content needs to be one of yum, file, ansible_collection, docker repositories.
        :type format: str
        :param from_history_id: Export history identifier used for incremental export. If not provided the most recent export history will be used.
        :type from_history_id: float
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """  # noqa: E501

        _param = self._post_content_export_incrementals_version_serialize(
            id=id,
            fail_on_missing_content=fail_on_missing_content,
            destination_server=destination_server,
            chunk_size_gb=chunk_size_gb,
            format=format,
            from_history_id=from_history_id,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index,
        )

        _response_types_map: Dict[str, Optional[str]] = {
            "200": None,
        }
        response_data = self.api_client.call_api(
            *_param, _request_timeout=_request_timeout
        )
        return response_data.response

    def _post_content_export_incrementals_version_serialize(
        self,
        id,
        fail_on_missing_content,
        destination_server,
        chunk_size_gb,
        format,
        from_history_id,
        _request_auth,
        _content_type,
        _headers,
        _host_index,
    ) -> Tuple:
        _host = None

        _collection_formats: Dict[str, str] = {}

        _path_params: Dict[str, str] = {}
        _query_params: List[Tuple[str, str]] = []
        _header_params: Dict[str, Optional[str]] = _headers or {}
        _form_params: List[Tuple[str, str]] = []
        _files: Dict[str, str] = {}
        _body_params: Optional[bytes] = None

        # process the path parameters
        # process the query parameters
        # process the header parameters
        # process the form parameters
        if id is not None:
            _form_params.append(("id", id))
        if fail_on_missing_content is not None:
            _form_params.append(("fail_on_missing_content", fail_on_missing_content))
        if destination_server is not None:
            _form_params.append(("destination_server", destination_server))
        if chunk_size_gb is not None:
            _form_params.append(("chunk_size_gb", chunk_size_gb))
        if format is not None:
            _form_params.append(("format", format))
        if from_history_id is not None:
            _form_params.append(("from_history_id", from_history_id))
        # process the body parameter

        # set the HTTP header `Content-Type`
        if _content_type:
            _header_params["Content-Type"] = _content_type
        else:
            _default_content_type = self.api_client.select_header_content_type(
                ["application/x-www-form-urlencoded", "multipart/form-data"]
            )
            if _default_content_type is not None:
                _header_params["Content-Type"] = _default_content_type

        # authentication setting
        _auth_settings: List[str] = []

        return self.api_client.param_serialize(
            method="POST",
            resource_path="/content_export_incrementals/version",
            path_params=_path_params,
            query_params=_query_params,
            header_params=_header_params,
            body=_body_params,
            post_params=_form_params,
            files=_files,
            auth_settings=_auth_settings,
            collection_formats=_collection_formats,
            _host=_host,
            _request_auth=_request_auth,
        )
